{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Call Center Analytics using Waston AI Services\n\nThis notebook shows you", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Table of contents\n\n1. [Load the required libraries](#loadlibraries)\n2. [Load data from Cloud Object Storage](#loaddata)\n3. [Visualize Sentiment and Top Keywords using Watson NLU response](#visualizeNLU)\n4. [Visualize Emotion Tone using Watson Tone Analyzer response](#visualizeToneAnalyzer)\n5. [Summary](#summary)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"loadlibraries\"></a>\n## Step 1: Load the required libraries\n\n- <a href=\"https://github.com/amueller/word_cloud/\" target=\"_blank\" rel=\"noopener no referrer\">wordcloud</a> is a Python library for generating Word Clouds ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Run pip install only the first time, once installed on your Spark machine, no need to re-run unless you want to upgrade\n!pip install --upgrade --force-reinstall wordcloud"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import ibm_boto3\nfrom botocore.client import Config\nimport json\nimport pixiedust\nfrom pixiedust.display import *\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nimport matplotlib.pyplot as plt\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col"
        }, 
        {
            "source": "<a id=\"loaddata\"> </a>\n## Step 2: Load NLU enriched data from your Cloud Object Storage instance\n\nThe first step is to load the data. This notebook assumes you have your enriched data stored in cloud object storage. In particular, we load the Watson Natural Language Understanding response for call center logs from cloud object storage.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Set the credentials to a generic variable to reference in the rest of the notebook\ncos_credentials = credentials_1"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define Cloud Object Storage client by specifying the credentials for your COS instance\nclient = ibm_boto3.client(service_name='s3', \n    ibm_api_key_id=cos_credentials['IBM_API_KEY_ID'],\n    ibm_auth_endpoint=cos_credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=cos_credentials['ENDPOINT'])"
        }, 
        {
            "source": "<a id=\"visualizeNLU\"></a>\n## Step 3: Visualize Sentiment and Top Keywords using Watson NLU response\nDefine the function to parse Watson NLU json response and extract sentiment score, sentiment label, and keywords.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Method to parse NLU response file from Cloud Object Storage\n# and return sentiment score, sentiment label, and keywords\ndef getNLUresponse(COSclient, bucket, filename):\n    streaming_body = COSclient.get_object(Bucket=bucket, Key=filename)['Body']\n    nlu_response = json.loads(streaming_body.read().decode(\"utf-8\"))\n    if nlu_response and nlu_response['sentiment'] \\\n    and nlu_response['sentiment']['document'] and nlu_response['sentiment']['document']['label']:\n        sentiment_score = nlu_response['sentiment']['document']['score']\n        sentiment_label = nlu_response['sentiment']['document']['label']\n        keywords = list(nlu_response['keywords'])\n    else:\n        sentiment_score = 0.0\n        sentiment_label = None\n        keywords = null\n        \n    return (filename,sentiment_score,sentiment_label,keywords)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Read enriched files from Cloud Object Storage\n# Provide list of files saved in COS that include NLU response\nnlu_files=['sample1_nlu.json','sample2_nlu.json', 'sample3_nlu.json', 'sample4_nlu.json']\nnlu_header=['filename','sentiment_score','sentiment_label','keywords']\nnlu_results = []\nbucket = cos_credentials['BUCKET']\nfor filename in nlu_files:\n    print(\"Processing NLU response from file: \", filename)\n    nlu_results.append(getNLUresponse(client,bucket,filename))\n    "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(nlu_results)"
        }, 
        {
            "source": "Map the parsed NLU responses into a Spark dataframe, one record for each file, where each file is the NLU response for one call center record.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "callcenterlogs_nluDF = spark.createDataFrame(nlu_results, nlu_header)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# Common validation calls\nprint(type(callcenterlogs_nluDF))\ncallcenterlogs_nluDF.printSchema()\ncallcenterlogs_nluDF.show()"
        }, 
        {
            "source": "### Sentiment plots using PixieDust\nLeverage PixieDust to plot sentiment labels as a pie-chart showing how many positive, negative, and neutral calls are received.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "## Ignore any records with null sentiment label\ncallcenterlogs_nluDF = callcenterlogs_nluDF.where(col('sentiment_label').isNotNull())\nperlabel_sentimentDF = callcenterlogs_nluDF.groupBy('sentiment_label')\\\n                              .agg(F.count('filename')\\\n                              .alias('num_calls'))\n\n## Take a look\nperlabel_sentimentDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "tableFields": "sentiment_score", 
                        "no_margin": "true", 
                        "orientation": "vertical", 
                        "title": "sentiment distribution", 
                        "chartsize": "52", 
                        "mpld3": "false", 
                        "aggregation": "COUNT", 
                        "filter": "{}", 
                        "handlerId": "pieChart", 
                        "sortby": "Keys ASC", 
                        "keyFields": "sentiment_label"
                    }
                }, 
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# Call Pixiedust to visualize sentiment data\ndisplay(callcenterlogs_nluDF)"
        }, 
        {
            "source": "### Keywords visualization using Word Cloud\nNext, we process the NLU keywords results to understand what are the top keywords referenced in the call center interactions. This would be very helpful in delivering insights what are the main topics being referenced in these call center interactions.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import explode\n\n# Explode keywords\ncallcenterlogs_nluDF = callcenterlogs_nluDF.select(explode('keywords').alias('topkeywords'))\ncallcenterlogs_nluDF = callcenterlogs_nluDF.select('topkeywords').rdd.map(lambda row: row[0]).toDF()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "callcenterlogs_nluDF.head(4)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# UDF to return lower case of word\ndef toLowerCase(word):\n    return word.lower()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Process extracted keywords to change to lower case\nudfLowerCase = udf(toLowerCase, StringType())\ncallcenterlogsTopKeywordsDF = callcenterlogs_nluDF.withColumn('topkeywords',udfLowerCase('text'))\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Group by topkeywords and compute average relevance per keyword and also number of calls for each keyword\ncallcenterlogsKwdsNumDF = callcenterlogsTopKeywordsDF.groupBy('topkeywords')\\\n                              .agg(F.count('topkeywords').alias('kwdsnumcalls'))\ncallcenterlogsKwdsRelDF = callcenterlogsTopKeywordsDF.groupBy('topkeywords')\\\n                          .agg(F.avg('relevance').alias('kwdsavgrelevance'))\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# join the keywords nunber and keywords relevance dataframes into one\ncallcenterlogsKeywordsDF = callcenterlogsKwdsNumDF.join(callcenterlogsKwdsRelDF,'topkeywords','outer')\n\n# Define keyword score as product of number of calls expressing that keyword and average relevance of that keyword\ncallcenterlogsKeywordsDF = callcenterlogsKeywordsDF.withColumn('keyword_score',callcenterlogsKeywordsDF.kwdsnumcalls * callcenterlogsKeywordsDF.kwdsavgrelevance)\n\n# Sort dataframe in descending order of KEYWORD_SCORE\ncallcenterlogsKeywordsDF = callcenterlogsKeywordsDF.orderBy('keyword_score',ascending=False)\n\n# Remove None keywords\ncallcenterlogsKeywordsDF = callcenterlogsKeywordsDF.where(col('topkeywords').isNotNull())\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(\"Top Keywords from call center logs\")\ncallcenterlogsKeywordsDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "barChart", 
                        "valueFields": "kwdsnumcalls", 
                        "keyFields": "topkeywords", 
                        "orientation": "horizontal"
                    }
                }
            }, 
            "outputs": [], 
            "source": "display(callcenterlogsKeywordsDF)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Map to Pandas DataFrame\ncallcenterlogsKeywordsPandas = callcenterlogsKeywordsDF.toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from wordcloud import WordCloud\n\n# Process Pandas DataFrame in the right format to leverage wordcloud.py for plotting\n# See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py \ndef prepForWordCloud(pandasDF,n):\n    kwdList = pandasDF['topkeywords']\n    sizeList = pandasDF['keyword_score']\n    kwdSize = {}\n    for i in range(n):\n        kwd=kwdList[i]\n        size=sizeList[i]\n        kwdSize[kwd] = size\n    return kwdSize"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%matplotlib inline\nmaxWords = len(callcenterlogsKeywordsPandas)\nnWords = 4\n\n#Generating wordcloud. Relative scaling value is to adjust the importance of a frequency word.\n#See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py\ncallcenterlogsKwdFreq = prepForWordCloud(callcenterlogsKeywordsPandas,nWords)\ncallcenterlogsWordCloud = WordCloud(max_words=maxWords,relative_scaling=0,normalize_plurals=False).generate_from_frequencies(callcenterlogsKwdFreq)\n\nplt.imshow(callcenterlogsWordCloud)\n\n# If need to support multiple side-by-side word clouds, use commented lines below\n\n#fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (23, 10))\n\n## Set titles for images\n#ax[0].set_title('Top Keywords from logs of call 1')\n#ax[1].set_title('Top Keywords from logs of call 2')\n\n                \n## Plot word clouds\n#ax[0].imshow(callcenterlogs1WordCloud)\n#ax[1].imshow(callcenterlogs2WordCloud)\n\n# turn off axis and ticks\n#plt.axis(\"off\")\n#ax[0].tick_params(axis='both',which='both',bottom='off',top='off',left='off',right='off',\n#                 labelbottom='off',labeltop='off',labelleft='off',labelright='off') \n#ax[1].tick_params(axis='both',which='both',bottom='off',top='off',left='off',right='off',\n#                 labelbottom='off',labeltop='off',labelleft='off',labelright='off') \n\n\n#plt.show()"
        }, 
        {
            "source": "<a id=\"visualizeToneAnalyzer\"></a>\n## Step 4: Visualize Emotion Tone using Watson Tone Analyzer response\nDefine the function to parse Watson Tone Analyzer json response and extract emotion tone labels and scores.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Method to parse Tone Analyzer response file from Cloud Object Storage\n# and return emotion tone labels and scores\ntoneID_list=['excited','frustrated','impolite','polite','sad','satisfied','sympathetic']\ndef getTAresponse(COSclient, bucket, filename):\n    streaming_body = COSclient.get_object(Bucket=bucket, Key=filename)['Body']\n    ta_response = json.loads(streaming_body.read().decode(\"utf-8\"))\n    if ta_response and ta_response['utterances_tone']:\n        # Assume one set of tones per file; if file is created to include a number of utterances\n        # we will need to change this code\n        tones = ta_response['utterances_tone'][0]['tones']\n    else:\n        tones = []\n    return (filename, tones)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toneanalyzer_files=['sample1_ta.json','sample2_ta.json']\ntoneanalyzer_header=['filename','tones']\ntoneanalyzer_results = []\nbucket = cos_credentials['BUCKET']\nfor filename in toneanalyzer_files:\n    print(\"Processing Tone Analyzer response from file: \", filename)\n    response= getTAresponse(client,bucket,filename)\n    toneanalyzer_results.append(response)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "callcenterlogs_taDF = spark.createDataFrame(toneanalyzer_results, toneanalyzer_header)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "callcenterlogs_taDF.head(4)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "callcenterlogs_taDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# If not imported earlier, import explode\nfrom pyspark.sql.functions import explode\n\n# Explode keywords\ncallcenterlogs_taDF = callcenterlogs_taDF.select(explode('tones').alias('toptones'))\ncallcenterlogs_taDF = callcenterlogs_taDF.select('toptones').rdd.map(lambda row: row[0]).toDF()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print schema and note that score is of type string\ncallcenterlogs_taDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Cast the score column from String to Double\ncallcenterlogs_taDF = callcenterlogs_taDF.withColumn(\"score\", col(\"score\").cast(\"double\"))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print schema to verify score is now of type double\ncallcenterlogs_taDF.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "callcenterlogs_taDF.head(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "tone_id", 
                        "chartsize": "51", 
                        "valueFields": "score", 
                        "handlerId": "pieChart", 
                        "aggregation": "AVG"
                    }
                }
            }, 
            "outputs": [], 
            "source": "display(callcenterlogs_taDF)"
        }, 
        {
            "source": "### Stop Here\nCells below are not needed when using PixieDust as it simplifies much of the processing executed in the steps below.\n\nHowever, we're keeping them here commented for reference, in case you want to explore other operations.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Group by toptones and compute average score per tone and also number of calls for each tone\ncallcenterlogsTonesNumDF = callcenterlogs_taDF.groupBy('tone_id')\\\n                           .agg(F.count('tone_id').alias('tonesnumcalls'))\ncallcenterlogsTonesScoreDF = callcenterlogs_taDF.groupBy('tone_id')\\\n                          .agg(F.avg('score').alias('tonesavgscore'))\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# join the tones nunber and tones scores dataframes into one\ncallcenterlogsTonesDF = callcenterlogsTonesNumDF.join(callcenterlogsTonesScoreDF,'tone_id','outer')\n\n# Define tones score as product of number of calls expressing that tone and average score of that tone\ncallcenterlogsTonesDF = callcenterlogsTonesDF.withColumn('tones_score',callcenterlogsTonesDF.tonesnumcalls * callcenterlogsTonesDF.tonesavgscore)\n\n# Sort dataframe in descending order of tones_score\ncallcenterlogsTonesDF = callcenterlogsTonesDF.orderBy('tones_score',ascending=False)\n\n# Remove None tones\ncallcenterlogsTonesDF = callcenterlogsTonesDF.where(col('tone_id').isNotNull())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#response= getTAresponse(client,bucket,filename)\n#print(json.dumps(resp, indent=4, sort_keys=True))"
        }, 
        {
            "source": "<a id=\"summary\"></a>\n## Summary\nWrite summary here", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}