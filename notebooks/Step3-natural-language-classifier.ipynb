{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "\n# Notebook 3 \u2013 Natural Language Classifier (NLC)\nIBM Watson Natural Language Classifier uses machine learning algorithms to return the top matching predefined classes for short text input. \n\n*YOU* Create and train a classifier to connect predefined classes to example texts so that the service can apply those classes to new inputs.\n\nhttps://www.ibm.com/watson/services/natural-language-classifier/ \nhttps://www.ibm.com/watson/developercloud/natural-language-classifier/api/v1 \n\n\n## Install dependencies", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#imports.... Run this each time after restarting the Kernel\n#!pip install watson_developer_cloud\nimport watson_developer_cloud as watson\nimport json\nfrom botocore.client import Config\nimport ibm_boto3\nimport requests"
        }, 
        {
            "source": "### Create Watson Natural Language Classifier service\n\n\n### Add Credentials\n\nCopy paste the following snippet to next cell, and add your own set of crdentials there:\n\n```code\ncredentials_os = {\n  \"apikey\": \"\",\n  \"cos_hmac_keys\": {\n    \"access_key_id\": \"\",\n    \"secret_access_key\": \"\"\n  },\n  \"endpoints\": \"\",\n  \"iam_apikey_description\": \"\",\n  \"iam_apikey_name\": \"\",\n  \"iam_role_crn\": \"\",\n  \"iam_serviceid_crn\": \"\",\n  \"resource_instance_id\": \"\"\n}\n\ncredentials_os['BUCKET'] = '<bucket_name_from_your_COS' # copy bucket name from COS\n\n\ncredentials_nlc = {\n    \"classifier_id\": \"\",\n    \"url\": \"\",\n    \"username\": \"\",\n    \"password\": \"\"\n}\n\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## TRAIN the NLC by sending it a ground_truth.CSV file to process\n\n<< TO DO MAMOON >> ADD LOGIC HERE FOR PYTHON UPLOAD, PRINT THE MODEL-ID (returned) AND THEN CHECK STATUS (when ready) >>\nthis can take 10m for small ground truth CSV's and longer for more complex - for the tutorial - you can come back later for yours, or use ours\n\n### Not ready msg  >>  \"The classifier instance is in its training phase, not yet ready to accept classify requests\"\n### Ready message >> \"The classifier instance is now available and is ready to take classifier requests\"\n\n### Classifier Training?  Waiting?  No problem - for lab we've pre-trained NLC to be ready to interrogate immediately - with creds", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Temp BURNER - \"call_center_gt_NLC_V2.csv\"\n# Credentials will only be available till March 23, 2018; afterward you need to train your own classifier\ncredentials_nlc = {\n    \"classifier_id\": \"f7ea68x308-nlc-917\",\n    \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api\",\n    \"username\": \"280b9633-d8c0-4ed2-9ee6-1b2c139516fb\",\n    \"password\": \"xeDbLU87jHZZ\"\n}\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "endpoints = requests.get(credentials_os['endpoints']).json()\n\niam_host = (endpoints['identity-endpoints']['iam-token'])\ncos_host = (endpoints['service-endpoints']['cross-region']['us']['public']['us-geo'])\n\nauth_endpoint = \"https://\" + iam_host + \"/oidc/token\"\nservice_endpoint = \"https://\" + cos_host\n\n\nclient = ibm_boto3.client(\n    's3',\n    ibm_api_key_id = credentials_os['apikey'],\n    ibm_service_instance_id = credentials_os['resource_instance_id'],\n    ibm_auth_endpoint = auth_endpoint,\n    config = Config(signature_version='oauth'),\n    endpoint_url = service_endpoint\n   )\n\n\n"
        }, 
        {
            "source": "### NLC\n\n- `process_text()` goes throught the text and fetch sentences and concatenate transcript based on chunk size\n- `classify()` calls natural language classifier endpoint and classify the text fields in transcript\n- ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#NLC\n\nfrom watson_developer_cloud import NaturalLanguageClassifierV1\n\nnatural_language_classifier = NaturalLanguageClassifierV1(\n    username = credentials_nlc['username'],\n    password = credentials_nlc['password'])\n\nchunk_size = 25\n# Used to SPLIT up - \"CHUNK\" the aggregate transcript into smaller pieces\n\ndef chunk_transcript(transcript, chunk_size):\n    transcript = transcript.split(' ')\n    return [ transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size) ] # chunking data\n    \n\ndef process_text(text):\n    transcript=''\n    for sentence in json.loads(text)['results']:\n        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n    transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n    return transcript\n\ndef classify(file_name):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key = file_name.split('.')[0]+'_text.json')['Body']\n    transcript=streaming_body.read().decode(\"utf-8\")\n    analysis = {}\n    for chunk in process_text(transcript):\n        chunk = ' '.join(chunk)\n        analysis[chunk] = natural_language_classifier.classify(credentials_nlc['classifier_id'], chunk)\n    client.put_object(Bucket = credentials_os['BUCKET'], Key = file_name.split('.')[0]+'_nlc', Body= json.dumps(analysis))\n    return analysis\n\n\ndef classify_transcript(file_name):\n    status = natural_language_classifier.get_classifier(credentials_nlc['classifier_id'])\n    if status['status'] == 'Available':\n        classes = classify(file_name)\n    return classes\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_list = ['sample1-addresschange-positive.ogg',\n             'sample2-address-negative.ogg',\n             'sample3-shirt-return-weather-chitchat.ogg',\n             'sample4-angryblender-sportschitchat-recovery.ogg',\n             'sample5-calibration-toneandcontext.ogg',\n             'jfk_1961_0525_speech_to_put_man_on_moon.ogg',\n             'May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg'\n            ]\n\n# we add audio files to COS pre-conference - REMEMBER to update this if you add files in Notebook #1  (JSON here, OGG there)\n\nclassify_transcript(file_list[0])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "classify_transcript(file_list[6])\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}