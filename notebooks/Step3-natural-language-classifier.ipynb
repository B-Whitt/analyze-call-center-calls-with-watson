{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "\n# Notebook 3 \u2013 Natural Language Classifier (NLC)\nIBM Watson Natural Language Classifier uses machine learning algorithms to return the top matching predefined classes for short text input. \n\n*YOU* Create and train a classifier to connect predefined classes to example texts so that the service can apply those classes to new inputs.\n\nhttps://www.ibm.com/watson/services/natural-language-classifier/ \nhttps://www.ibm.com/watson/developercloud/natural-language-classifier/api/v1 \n\n\n## Install dependencies", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#imports.... Run this each time after restarting the Kernel\n#!pip install watson_developer_cloud\nimport watson_developer_cloud as watson\nimport json\nfrom botocore.client import Config\nimport ibm_boto3\nimport requests"
        }, 
        {
            "source": "##  Cloud Object Storage - Add Credentials & Bucket Name\nIf you've not already set up COS - please see Step 1\n\n### Credentials\nCredentials are also created for you when you create project. From service dashboard page select `Service Credentials` from left navigation menu item, and copy/paste the credentials below:\n\n### Bucket name\nBuckets are created for you when you create project. From service dashboard page select `Buckets` from left navigation menu item, and get your bucket name and copy/paste bucket name below:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# For Cloud Object Storage - populate your own information here from \"SERVICES\" on this page, or Console Dashboard on ibm.com/cloud\n\n# From service dashboard page select Service Credentials from left navigation menu item\ncredentials_os = {\n  \"apikey\": \"\",\n  \"cos_hmac_keys\": {\n    \"access_key_id\": \"\",\n    \"secret_access_key\": \"\"\n  },\n  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance\",\n  \"iam_apikey_name\": \"\",\n  \"iam_role_crn\": \"\",\n  \"iam_serviceid_crn\": \"\",\n  \"resource_instance_id\": \"\"\n}\n\n# Buckets are created for you when you create project. From service dashboard page select Buckets from left navigation menu item, \ncredentials_os['BUCKET'] = '<bucket_name>' # copy bucket name from COS"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "### ACCESS (pre-trained) Watson Natural Language Classifier (NLC) service for lab\n### (*) NLC does NOT OFFER LITE PLAN and NLC also takes time to train\n\nFor this lab - to keep things simple - NLC has been PRE CONFIGURED for you. \n\nIBM Watson\u2122 Natural Language Classifier uses machine learning algorithms to return the top matching predefined classes for short text input. You create and train a classifier to connect predefined classes to example texts so that the service can apply those classes to new inputs.\n\nIn short - YOU can train the NLC with a ground truth - to create your own classification model\n\nhttps://www.ibm.com/watson/developercloud/natural-language-classifier/api/v1/curl.html?curl\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# LAB CREDENTIALS FOR YOU - Credentials will only be available till March 23, 2018; afterward you need to train your own classifier\n\ncredentials_nlc = {\n    \"classifier_id\": \"f7ea68x308-nlc-917\",\n    \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api\",\n    \"username\": \"280b9633-d8c0-4ed2-9ee6-1b2c139516fb\",\n    \"password\": \"xeDbLU87jHZZ\"\n}\n# Ground truth used - simple tester \"call_center_gt_NLC_V2.csv\"\n# https://github.com/rustyoldrake/call_center_instrumentation_analytics/blob/master/call_center_gt_NLC_V2.csv"
        }, 
        {
            "source": "### Set up Object Storage Client", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "endpoints = requests.get(credentials_os['endpoints']).json()\n\niam_host = (endpoints['identity-endpoints']['iam-token'])\ncos_host = (endpoints['service-endpoints']['cross-region']['us']['public']['us-geo'])\n\nauth_endpoint = \"https://\" + iam_host + \"/oidc/token\"\nservice_endpoint = \"https://\" + cos_host\n\n\nclient = ibm_boto3.client(\n    's3',\n    ibm_api_key_id = credentials_os['apikey'],\n    ibm_service_instance_id = credentials_os['resource_instance_id'],\n    ibm_auth_endpoint = auth_endpoint,\n    config = Config(signature_version='oauth'),\n    endpoint_url = service_endpoint\n   )\n\n\n"
        }, 
        {
            "source": "### NLC\n\n- `process_text()` goes throught the text and fetch sentences and concatenate transcript based on chunk size\n- `classify()` calls natural language classifier endpoint and classify the text fields in transcript", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#NLC\n\nfrom watson_developer_cloud import NaturalLanguageClassifierV1\n\nnatural_language_classifier = NaturalLanguageClassifierV1(\n    username = credentials_nlc['username'],\n    password = credentials_nlc['password'])\n\nchunk_size = 25\n# Used to SPLIT up - \"CHUNK\" the aggregate transcript into smaller pieces\n\ndef chunk_transcript(transcript, chunk_size):\n    transcript = transcript.split(' ')\n    return [ transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size) ] # chunking data\n    \n\ndef process_text(text):\n    transcript=''\n    for sentence in json.loads(text)['results']:\n        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n    transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n    return transcript\n\ndef classify(file_name):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key = file_name.split('.')[0]+'_text.json')['Body']\n    transcript=streaming_body.read().decode(\"utf-8\")\n    analysis = {}\n    for chunk in process_text(transcript):\n        chunk = ' '.join(chunk)\n        analysis[chunk] = natural_language_classifier.classify(credentials_nlc['classifier_id'], chunk)\n    client.put_object(Bucket = credentials_os['BUCKET'], Key = file_name.split('.')[0]+'_nlc', Body= json.dumps(analysis))\n    return analysis\n\n\ndef classify_transcript(file_name):\n    status = natural_language_classifier.get_classifier(credentials_nlc['classifier_id'])\n    if status['status'] == 'Available':\n        classes = classify(file_name)\n    return classes\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_list = ['sample1-addresschange-positive.ogg',\n             'sample2-address-negative.ogg',\n             'sample3-shirt-return-weather-chitchat.ogg',\n             'sample4-angryblender-sportschitchat-recovery.ogg',\n             'sample5-calibration-toneandcontext.ogg',\n             'jfk_1961_0525_speech_to_put_man_on_moon.ogg',\n             'May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg'\n            ]\n\n\nclassify_transcript(file_list[0])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for filename in file_list:\n    print(\"\\n\\nprocessing file: \", filename)\n    analysis = classify_transcript(filename)\n    print(analysis)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}