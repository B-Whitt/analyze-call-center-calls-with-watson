{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "\n## Notebook 2 \u2013 Natural Language Understanding (NLU)\nNLU analyzes text to extract meta-data from content such as concepts, entities, keywords, categories, relations and semantic roles.\nhttps://www.ibm.com/watson/services/natural-language-understanding/ \nhttps://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/  \n\n\n## Install dependencies\n\nPython\u2019s standard library is very extensive, offering a wide range of facilities. It contains built-in modules like JSON a lightweight data interchange format. https://docs.python.org/2/library/index.html and https://docs.python.org/2/library/json.html\n\nIBM Watson Developer Cloud has a Python client library to quickly get started with the various Watson APIs services. https://pypi.python.org/pypi/watson-developer-cloud\n\nUsing Python with IBM COS: Python support is provided through the Boto 3 library. The boto3 library provides complete access and can source credentials. The IBM COS endpoint must be specified when creating a service resource or low-level client as shown in documentation https://ibm-public-cos.github.io/crs-docs/python\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#imports.... Run this each time after restarting the Kernel\n#!pip install watson_developer_cloud\nimport watson_developer_cloud as watson\nimport json\nfrom botocore.client import Config\nimport ibm_boto3\nimport requests\n"
        }, 
        {
            "source": "### Create Watson Natural Language Understanding service\n\nFor more information on creating Watson services, see Notebook 1\n\n### Add Credentials\n\nCopy paste the following snippet to next cell, and add your own set of crdentials there:\n\n```code\ncredentials_os = {\n  \"apikey\": \"\",\n  \"cos_hmac_keys\": {\n    \"access_key_id\": \"\",\n    \"secret_access_key\": \"\"\n  },\n  \"endpoints\": \"\",\n  \"iam_apikey_description\": \"\",\n  \"iam_apikey_name\": \"\",\n  \"iam_role_crn\": \"\",\n  \"iam_serviceid_crn\": \"\",\n  \"resource_instance_id\": \"\"\n}\ncredentials_os['BUCKET'] = '<bucket_name_from_your_COS_instance>'\n\ncredentials_nlu = {\n    \"url\": \"\",\n    \"username\": \"\",\n    \"password\": \"\",\n    \"version\": \"2017-02-27\"\n}\n\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "## Set-up Object storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "endpoints = requests.get(credentials_os['endpoints']).json()\n\niam_host = (endpoints['identity-endpoints']['iam-token'])\ncos_host = (endpoints['service-endpoints']['cross-region']['us']['public']['us-geo'])\n\nauth_endpoint = \"https://\" + iam_host + \"/oidc/token\"\nservice_endpoint = \"https://\" + cos_host\n\n\nclient = ibm_boto3.client(\n    's3',\n    ibm_api_key_id = credentials_os['apikey'],\n    ibm_service_instance_id = credentials_os['resource_instance_id'],\n    ibm_auth_endpoint = auth_endpoint,\n    config = Config(signature_version='oauth'),\n    endpoint_url = service_endpoint\n   )\n\n\n"
        }, 
        {
            "source": "### NLU\n\n- `process_text()` goes throught the text and fetch sentences and concatenate transcript based on chunk size\n- `analyze transcript()` calls natural language understanding endpoint and analyze the transcripe\n- `post_analysis` processes the results and show insights based on response from NLU endpoint", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#NLU\n\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding.features import (\n    v1 as Features)\n\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version = '2017-02-27',\n    username = credentials_nlu['username'],\n    password = credentials_nlu['password']\n)\n\nchunk_size=25 # This CHUNK size is used to disaggregate a transcript \n#e.g. in this case a 290 word transcript would have 10 chunks - 9 with 30 words and 1 with 20 words - approximates 'time domain' for this lab\n\ndef chunk_transcript(transcript, chunk_size):\n    transcript = transcript.split(' ')\n    return [ transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size) ] # chunking data\n\ndef process_text(text):\n    transcript=''\n    for sentence in json.loads(text)['results']:\n        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n    #transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n    return  transcript\n\n\ndef analyze_transcript(features, file_name):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_text.json')['Body']\n    transcript = process_text(streaming_body.read().decode(\"utf-8\"))\n    nlu_analysis = natural_language_understanding.analyze(features, transcript, return_analyzed_text=True)\n    res=client.put_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_NLU.json', Body= json.dumps(nlu_analysis))\n    return nlu_analysis\n\ndef post_analysis(result):\n    print(result['analyzed_text'])\n    categories = result['categories']\n    for category in categories:\n        print('label: ', category['label'], ', score: ', category['score']) #add table instead of prints\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_list = ['sample1-addresschange-positive.ogg',\n             'sample2-address-negative.ogg',\n             'sample3-shirt-return-weather-chitchat.ogg',\n             'sample4-angryblender-sportschitchat-recovery.ogg',\n             'sample5-calibration-toneandcontext.ogg',\n             'jfk_1961_0525_speech_to_put_man_on_moon.ogg',\n             'May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg']\n\nfeatures = {\"concepts\":{},\"entities\":{},\"keywords\":{},\"categories\":{},\"emotion\":{},\"sentiment\":{},\"semantic_roles\":{} }"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result = analyze_transcript(features, file_list[0])\npost_analysis(result)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "results = analyze_transcript(features, file_list[6])\n\npost_analysis(results)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}