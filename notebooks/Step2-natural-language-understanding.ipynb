{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "\n## Notebook 2 \u2013 Natural Language Understanding (NLU)\nNLU analyzes text to extract meta-data from content such as concepts, entities, keywords, categories, relations and semantic roles.\nhttps://www.ibm.com/watson/services/natural-language-understanding/ \nhttps://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/  \n\n\n## Install dependencies\n\nPython\u2019s standard library is very extensive, offering a wide range of facilities. It contains built-in modules like JSON a lightweight data interchange format. https://docs.python.org/2/library/index.html and https://docs.python.org/2/library/json.html\n\nIBM Watson Developer Cloud has a Python client library to quickly get started with the various Watson APIs services. https://pypi.python.org/pypi/watson-developer-cloud\n\nUsing Python with IBM COS: Python support is provided through the Boto 3 library. The boto3 library provides complete access and can source credentials. The IBM COS endpoint must be specified when creating a service resource or low-level client as shown in documentation https://ibm-public-cos.github.io/crs-docs/python\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 84, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#imports.... Run this each time after restarting the Kernel\n#!pip install watson_developer_cloud\nimport watson_developer_cloud as watson\nimport json\nfrom botocore.client import Config\nimport ibm_boto3\n"
        }, 
        {
            "source": "### Create Watson Natural Language Understanding service\n\nFor more information on creating Watson services, see Notebook 1\n\n### Add Credentials\n\nCopy paste the following snippet to next cell, and add your own set of crdentials there:\n\n```code\ncredentials_os = {\n    'IBM_API_KEY_ID': '',\n    'IAM_SERVICE_ID': '',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n    'BUCKET': '',\n}\n\ncredentials_nlu = {\n    \"url\": \"\",\n    \"username\": \"\",\n    \"password\": \"\"\n}\n\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 85, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "## Set-up Object storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 86, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# For more information on creating Watson services, see Notebook 1\n\nclient = ibm_boto3.client(service_name='s3', \n    ibm_api_key_id=credentials_os['IBM_API_KEY_ID'],\n    ibm_auth_endpoint=credentials_os['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\n"
        }, 
        {
            "source": "### NLU\n\n- `process_text()` goes throught the text and fetch sentences and concatenate transcript based on chunk size\n- `analyze transcript()` calls natural language understanding endpoint and analyze the transcripe\n- `post_analysis` processes the results and show insights based on response from NLU endpoint", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 115, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#NLU\n\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding.features import (\n    v1 as Features)\n\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version = '2017-02-27',\n    username = credentials_nlu['username'],\n    password = credentials_nlu['password']\n)\n\nchunk_size=25 # This CHUNK size is used to disaggregate a transcript \n#e.g. in this case a 290 word transcript would have 10 chunks - 9 with 30 words and 1 with 20 words - approximates 'time domain' for this lab\n\ndef chunk_transcript(transcript, chunk_size):\n    transcript = transcript.split(' ')\n    return [ transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size) ] # chunking data\n\ndef process_text(text):\n    transcript=''\n    for sentence in json.loads(text)['results']:\n        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n    #transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n    return  transcript\n\n\ndef analyze_transcript(features, file_name):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_text.json')['Body']\n    transcript = process_text(streaming_body.read().decode(\"utf-8\"))\n    nlu_analysis = natural_language_understanding.analyze(features, transcript, return_analyzed_text=True)\n    res=client.put_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_NLU.json', Body= json.dumps(nlu_analysis))\n    return nlu_analysis\n\ndef post_analysis(result):\n        print(result['analyzed_text'])\n        categories = result['categories']\n        for category in categories:\n            print('label: ', category['label'], ', score: ', category['score']) #add table instead of prints\n"
        }, 
        {
            "execution_count": 116, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_list = ['sample1-addresschange-positive.ogg',\n             'sample2-address-negative.ogg',\n             'sample3-shirt-return-weather-chitchat.ogg',\n             'sample4-angryblender-sportschitchat-recovery.ogg',\n             'sample5-calibration-toneandcontext.ogg',\n             'jfk_1961_0525_speech_to_put_man_on_moon.ogg',\n             'May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg']\n\nfeatures = {\"concepts\":{},\"entities\":{},\"keywords\":{},\"categories\":{},\"emotion\":{},\"sentiment\":{},\"semantic_roles\":{} }"
        }, 
        {
            "execution_count": 120, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "good morning can you give me some help I'd like to change my address please my name is Ryan Smith I am from Sacramento California that's right my phone number is five five five one two one two yes that's me my old address is number one two three oak street my new address is five six seven pine street yes and the zip is nine zero two one zero yep that's right now the phone number stays the same that's right I would like to keep all the options of said no other changes the only thing that I want to change is the address yes that's right yep very good yes thank you so much for help it thanks have a good day bye bye \nlabel:  /business and industrial , score:  0.167796\nlabel:  /technology and computing/hardware/computer , score:  0.110389\nlabel:  /education/school , score:  0.102795\n"
                }
            ], 
            "source": "result = analyze_transcript(features, file_list[0])\npost_analysis(result)\n"
        }, 
        {
            "execution_count": 118, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "label:  /family and parenting/children , score:  0.218487\nlabel:  /business and industrial , score:  0.08215\nlabel:  /art and entertainment/movies and tv/television , score:  0.0816767\n"
                }
            ], 
            "source": "results = analyze_transcript(features, file_list[6])\n\npost_analysis(results)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}