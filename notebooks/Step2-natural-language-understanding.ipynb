{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "\n## Notebook 2 \u2013 Natural Language Understanding (NLU)\nNLU analyzes text to extract meta-data from content such as concepts, entities, keywords, categories, relations and semantic roles.\nhttps://www.ibm.com/watson/services/natural-language-understanding/ \nhttps://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/  \n\n\n## Install dependencies\n\nPython\u2019s standard library is very extensive, offering a wide range of facilities. It contains built-in modules like JSON a lightweight data interchange format. https://docs.python.org/2/library/index.html and https://docs.python.org/2/library/json.html\n\nIBM Watson Developer Cloud has a Python client library to quickly get started with the various Watson APIs services. https://pypi.python.org/pypi/watson-developer-cloud\n\nUsing Python with IBM COS: Python support is provided through the Boto 3 library. The boto3 library provides complete access and can source credentials. The IBM COS endpoint must be specified when creating a service resource or low-level client as shown in documentation https://ibm-public-cos.github.io/crs-docs/python\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 49, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#imports.... Run this each time after restarting the Kernel\n#!pip install watson_developer_cloud\nimport watson_developer_cloud as watson\nimport json\nfrom botocore.client import Config\nimport ibm_boto3\n"
        }, 
        {
            "source": "### Create Watson Natural Language Understanding service\n\nFor more information on creating Watson services, see Notebook 1\n\n### Add Credentials\n\nCopy paste the following snippet to next cell, and add your own set of crdentials there:\n\n```code\ncredentials_os = {\n    'IBM_API_KEY_ID': '',\n    'IAM_SERVICE_ID': '',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n    'BUCKET': '',\n}\n\ncredentials_nlu = {\n    \"url\": \"\",\n    \"username\": \"\",\n    \"password\": \"\"\n}\n\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 50, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "## Set-up Object storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 51, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# For more information on creating Watson services, see Notebook 1\n\nclient = ibm_boto3.client(service_name='s3', \n    ibm_api_key_id=credentials_os['IBM_API_KEY_ID'],\n    ibm_auth_endpoint=credentials_os['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\n"
        }, 
        {
            "source": "### NLU\n\n- `process_text()` goes throught the text and fetch sentences and concatenate transcript based on chunk size\n- `analyze transcript()` calls natural language understanding endpoint and analyze the transcripe\n- `post_analysis` processes the results and show insights based on response from NLU endpoint", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#NLU\n\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding.features import (\n    v1 as Features)\n\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version = '2017-02-27',\n    username = credentials_nlu['username'],\n    password = credentials_nlu['password'])\n\nchunk_size=25 # This CHUNK size is used to disaggregate a transcript \n#e.g. in this case a 290 word transcript would have 10 chunks - 9 with 30 words and 1 with 20 words - approximates 'time domain' for this lab\n\ndef chunk_transcript(transcript, chunk_size):\n    transcript = transcript.split(' ')\n    return [ transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size) ] # chunking data\n\ndef process_text(text):\n    transcript=''\n    for sentence in json.loads(text)['results']:\n        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n    transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n    return transcript\n\n\ndef analyze_transcript(features, file_name):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_text.json')['Body']\n    transcript=streaming_body.read().decode(\"utf-8\")\n    nlu_analysis={}\n    for chunk in process_text(transcript):\n        if len(chunk) > 5:\n            chunk = ' '.join(chunk)\n            nlu_analysis[chunk] = natural_language_understanding.analyze(features, chunk, return_analyzed_text=True)\n    res=client.put_object(Bucket = credentials_os['BUCKET'], Key=file_name[0].split('.')[0]+'_NLU.json', Body= json.dumps(nlu_analysis))\n    return nlu_analysis\n\ndef post_analysis(result):\n    for chunk in result.keys():\n        categories = result[chunk]['categories']\n        print('\\nchunk: ', chunk)\n        for category in categories:\n            print('label: ', category['label'], ', score: ', category['score']) #add table instead of prints\n"
        }, 
        {
            "execution_count": 70, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_list = ['sample1-addresschange-positive.ogg',\n             'sample2-address-negative.ogg',\n             'sample3-shirt-return-weather-chitchat.ogg',\n             'sample4-angryblender-sportschitchat-recovery.ogg',\n             'sample5-calibration-toneandcontext.ogg',\n             'jfk_1961_0525_speech_to_put_man_on_moon.ogg',\n             'May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg']\n\nfeatures = {\"concepts\":{},\"entities\":{},\"keywords\":{},\"categories\":{},\"emotion\":{},\"sentiment\":{},\"semantic_roles\":{} }"
        }, 
        {
            "execution_count": 71, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\nchunk:  of said no other changes the only thing that I want to change is the address yes that's right yep very good yes thank you so much for help it\nlabel:  /shopping/resources/contests and freebies , score:  0.226511\nlabel:  /law, govt and politics/legal issues/legislation/tax laws , score:  0.16996\nlabel:  /business and industrial , score:  0.169616\n\nchunk:  is five five five one two one two yes that's me my old address is number one two three oak street my new address is five six seven pine street\nlabel:  /business and industrial , score:  0.22775\nlabel:  /real estate/apartments , score:  0.147439\nlabel:  /travel/tourist facilities/hotel , score:  0.129948\n\nchunk:  yes and the zip is nine zero two one zero yep that's right now the phone number stays the same that's right I would like to keep all the options\nlabel:  /technology and computing/consumer electronics/telephones/mobile phones , score:  0.165103\nlabel:  /travel/tourist destinations/mexico and central america , score:  0.142812\nlabel:  /business and industrial , score:  0.13298\n\nchunk:  good morning can you give me some help I'd like to change my address please my name is Ryan Smith I am from Sacramento California that's right my phone number\nlabel:  /technology and computing/hardware/computer , score:  0.798642\nlabel:  /finance/personal finance/lending/credit cards , score:  0.416438\nlabel:  /education/school , score:  0.215977\n\nchunk:  thanks have a good day bye bye \nlabel:  /art and entertainment/theatre , score:  0.969809\nlabel:  /art and entertainment/dance , score:  0.140806\nlabel:  /art and entertainment/music , score:  0.11456\n"
                }
            ], 
            "source": "result = analyze_transcript(features, file_list[0])\n\npost_analysis(result)\n"
        }, 
        {
            "execution_count": 72, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "WatsonApiException", 
                    "evalue": "Error: invalid request: content is empty, Code: 400", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mWatsonApiException\u001b[0m                        Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-72-17a8e110803b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpost_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m<ipython-input-69-2b34ece0f3b3>\u001b[0m in \u001b[0;36manalyze_transcript\u001b[0;34m(features, file_name)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mnlu_analysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatural_language_understanding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_analyzed_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcredentials_os\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BUCKET'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_NLU.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlu_analysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlu_analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m~/.local/lib/python3.5/site-packages/watson_developer_cloud/natural_language_understanding_v1.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, features, text, html, url, clean, xpath, fallback_to_raw, return_analyzed_text, language, limit_text_characters)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/v1/analyze'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         response = self.request(\n\u001b[0;32m--> 174\u001b[0;31m             method='POST', url=url, params=params, json=data, accept_json=True)\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m~/.local/lib/python3.5/site-packages/watson_developer_cloud/watson_service.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, accept_json, headers, params, json, data, files, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0merror_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_error_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             raise WatsonApiException(response.status_code, error_message,\n\u001b[0;32m--> 415\u001b[0;31m                                      error_info)\n\u001b[0m", 
                        "\u001b[0;31mWatsonApiException\u001b[0m: Error: invalid request: content is empty, Code: 400"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "results = analyze_transcript(features, file_list[6])\n\npost_analysis(results)\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}