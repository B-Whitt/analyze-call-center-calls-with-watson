{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Call Center Instrumentation & Analytics (CCIA)\n\n## Watson-Call-Center-Think18 Lab March 2015\n\nThis document provides guidance and background for a hands-on Python + IBM Watson lab being presented at an IBM Think2018 conference in March 2018, and for an IPython / Jupyter notebook and python code available for open source after the event.\n\nThe focus is Call Center Instrumentation and Analytics (CCIA) pattern.   The Notebook and information here seek to help organizations beginning to explore how to better understand the unstructured \"dark data\" that arises from phone calls to call centers. \n\n### Why is this useful?\nEnterprises spend more than $1 trillion on 250 billion customer service calls each year.  By using multiple IBM Watson \"signal services\" to extract signal from raw audio data; perform data analytics, clustering, unsupervised and machine learning, and visualizations, technical teams can use data understand patterns in call centers. KPI and ROI positive.\n\n### What is the process? And what Watson services are used?\nStep 1 - Speech to Text (STT) \u2013 Converts Raw Audio to Transcripts; \nStep 2 - Natural Language Understanding (NLU) - extracts features concepts, entities, keywords, categories/topics, sentiment and emotion; \nStep 3 - Natural Language Classifier (NLC) - is a user trained classification service, with user defined \u201cground truth\u201d that classifies text chunks; \nStep 4 - Tone Analyzer (Tone) \u2013 uses linguistic analysis to detect emotional and language tones in written text; \nStep 5 - Call Center Analytics \u2013 analyzes and visualizes the data signal to allow for interpretation of data and in cases, actionable insights; \n\n\n### Beginner Audience & Focus on Basics\n\u2022\tThis is a beginner lab intended to educate on the fundamentals of getting from data to insights with IBM Watson and open source tools \n\u2022\tAudience may include IT and operations teams curious about enriching unstructured data \u2013 the lab is NOT intended for sophisticated call center technologists \n\u2022\tLab/code does NOT purport to compete with expensive and sophisticated solutions already in market \n\u2022\tThe lab and code cover the basics \u2013 to educate on the fundamental plumbing and steps, to provide base for instrumentation \n\n### Success Metrics\nIf successful \u2013 the lab participants or notebook users will\n1.\tGain experience in using an IPython / Jupyter notebook\nhttps://ipython.org/notebook.html\n2.\tConnect to four Watson Developer Cloud \u2018signal service\u2019 APIs \nhttps://www.ibm.com/watson/developer/ \n3.\tConnect to IBM Cloud Object storage for data read and write \nhttps://www.ibm.com/cloud/object-storage \n4.\tUnderstand whether/how the tools and methods might benefit org\nhttps://github.com/mamoonraja/call-center-think18/tree/master/notebooks\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Notebook 1 \u2013 Speech to Text (STT) & First Contact\n## Install Python Dependencies\n\nPython\u2019s standard library is very extensive, offering a wide range of facilities.  It contains built-in modules like JSON a lightweight data interchange format.  https://docs.python.org/2/library/index.html and https://docs.python.org/2/library/json.html\n\nIBM Watson Developer Cloud has a Python client library to quickly get started with the various Watson APIs services.\nhttps://pypi.python.org/pypi/watson-developer-cloud\n\nUsing Python with IBM COS: Python support is provided through the Boto 3 library. The boto3 library provides complete access and can source credentials. The IBM COS endpoint must be specified when creating a service resource or low-level client as shown in documentation\nhttps://ibm-public-cos.github.io/crs-docs/python\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#imports.... Run this each time after restarting the Kernel\n#!pip install watson_developer_cloud\nimport watson_developer_cloud as watson\nimport json\nfrom botocore.client import Config\nimport ibm_boto3\n"
        }, 
        {
            "source": "### Add COS and STT Credentials\n\nCopy paste the following snippet to next cell, and add your own set of credentials  \n\nFor Cloud Object Storage\n\n```code\ncredentials_os = {\n    'IBM_API_KEY_ID': '',\n    'IAM_SERVICE_ID': '',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n    'BUCKET': '',\n}\n```\n\nFor IBM Watson Speech to Text (STT) API\n\n```code\ncredentials_stt = {\n    \"url\": \"\",\n    \"username\": \"\",\n    \"password\": \"\"\n}\n\n```\n\n## Getting COS Credentials \nIBM Cloud Object Storage is a highly scalable cloud storage service, designed for high durability, resiliency and security. Store, manage and access your data via our self-service portal and RESTful APIs. Connect applications directly to Cloud Object Storage use other IBM Cloud Services with your data.\nCreate LITE (free) service here https://console.bluemix.net/catalog/services/cloud-object-storage and get credentials by clicking on service credentials and then \u201cview credential\u201d \n\n\n## Getting STT Credentials \nImporting Credentials \u2013 Each Watson signal service (STT, NLC, NLU and Tone) will require credentials - a username and password\nIf you already have an IBM Cloud / Bluemix account login here https://console.bluemix.net/ but if you have not yet registered for IBM Cloud - you will need to Register for a Free account here https://www.ibm.com/watson/developer/ registration takes less than 4 minutes and is free. More information here https://www.ibm.com/watson/developer-resources/ \n\nOnce logged in - go to https://console.bluemix.net/developer/watson/dashboard - browse services for SPEECH TO TEXT, and select Details, Create service from here https://console.bluemix.net/catalog/services/speech-to-text  - for free you can select LITE Plan \nLITE plan for STT \u201cgets you started with 100 minutes per month at no cost\u201d\n\nThe Username and Password (and URL) is found by clicking on service credentials and then \u201cview credential\u201d \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = ibm_boto3.client(\n    service_name = 's3', \n    ibm_api_key_id = credentials_os['IBM_API_KEY_ID'],\n    ibm_auth_endpoint = credentials_os['IBM_AUTH_ENDPOINT'],\n    config = Config(signature_version = 'oauth'),\n    endpoint_url = 'https://s3-api.us-geo.objectstorage.service.networklayer.com'\n)\n"
        }, 
        {
            "source": "## Speect to Text \n\nFollowing cell has two methods:\n - `get_transcript()` calls speech to text enpoint and generates a text transcript for you for a sample audio file.\n - `analyze_sample()` gets the sample object from cloud storage, calls get_transcript to fetch the tranccript, and saves your transcript in cloud storage as `<file_name>_text.json`.\n \nOGG, WAV FLAC, L16, MP3, MPEG formats are options for the IBM Watson STT service.  For the lab we use OGG samples. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#STT\n\nimport json\nimport io\nfrom os.path import join, dirname\nfrom watson_developer_cloud import SpeechToTextV1\n\nspeech_to_text = SpeechToTextV1(\n    username = credentials_stt['username'],\n    password = credentials_stt['password'],\n    x_watson_learning_opt_out=False\n)\n\n\n# OGG, WAV FLAC, L16, MP3, MPEG formats are options for the STT service \n# with Narrowband (generaly telco) and Broadband (e.g. higher quality USB mic) audio.  \n# For the LAB \u2013 OGG format was used for sample files in lab. Of other audio formats e.g. WAV - remember to change 'OGG' content_type='audio/ogg' in code below if you do.\n\n#get transcript Very basic one\ndef get_transcript(audio):\n    transcript = json.dumps(speech_to_text.recognize(audio, content_type='audio/ogg', timestamps=True,\n        word_confidence=True), indent=2)\n    return transcript\n\ndef analyze_sample(sample):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=sample)['Body']\n    text = get_transcript(streaming_body.read())\n    client.put_object(Bucket = credentials_os['BUCKET'], Key = sample.split('.')[0] + '_text.json', Body = text)\n    return text\n\n"
        }, 
        {
            "source": "## Analyze the selected audio files\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "file_list = ['sample1-addresschange-positive.ogg',\n             'sample2-address-negative.ogg',\n             'sample3-shirt-return-weather-chitchat.ogg',\n             'sample4-angryblender-sportschitchat-recovery.ogg',\n             'sample5-calibration-toneandcontext.ogg',\n             'jfk_1961_0525_speech_to_put_man_on_moon.ogg',\n             'May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg']\n\n# jfk_1961_0525_speech_to_put_man_on_moon.ogg  new and longer\n# May 1 1969 Fred Rogers testifies before the Senate Subcommittee on Communications.ogg\n\n\n# FILE LIST \u2013 in this notebook, each OGG file produces its own transcript.  These ones are quite short so it happens quickly.  \n# Longer (e.g. 1 hour) audio files may justify using asynchronous method, and a real time a sessions method (both defined below)\n\n# For longer files and transcription at scale: https://www.ibm.com/watson/developercloud/speech-to-text/api/v1/  \n# WebSockets includes a single method that establishes a persistent connection with the service over the WebSocket protocol.\n# Sessionless includes a method that provides a simple means of transcribing audio without the overhead of establishing and maintaining a session.\n# Sessions provides methods that allow a client to maintain a long, multi-turn exchange, or session, with the service or to establish multiple parallel conversations with a particular instance of the service.\n# Asynchronous provides a non-blocking interface for transcribing audio. You can register a callback URL to be notified of job status and, optionally, results, or you can poll the service to learn job status and retrieve results manually.\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "good morning  0.995\ncan you give me some help I'd like to change my address please  0.999\nmy name is Ryan Smith  0.972\nI am from Sacramento California  0.969\nthat's right  0.996\nmy phone number is five five five one two one two  0.839\nyes that's me  0.997\nmy old address is number one two three oak street  0.965\nmy new address is five six seven pine street  0.869\nyes and the zip is nine zero two one zero  0.921\nyep that's right  0.89\nnow the phone number stays the same  0.808\nthat's right I would like to keep all the options of said no other changes the only thing that I want to change is the address  0.944\nyes that's right  0.995\nyep  0.583\nvery good yes thank you so much for help  0.921\nit  0.419\nthanks have a good day bye bye  0.994\n"
                }
            ], 
            "source": "# TRANSCRIBE \u2013 this is where STT receives the OGG files provided and returns text to TRANSCRIPT\n# this is a test of ONE transcription in the list - place '0' - may take a minute\ntranscript = analyze_sample(file_list[0])\n\nfor result in json.loads(transcript)['results']:\n    print(result['alternatives'][0]['transcript'], result['alternatives'][0]['confidence'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Testing position [6] - which is the LONGER OGG file - 7th position Fred Rogers - just under 7 minutes\n\ntranscript = analyze_sample(file_list[6])\n\nfor result in json.loads(transcript)['results']:\n    print(result['alternatives'][0]['transcript'], result['alternatives'][0]['confidence'])"
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "### analyze more samples and display results in better format (as table)?"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}