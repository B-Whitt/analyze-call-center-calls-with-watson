{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#imports.... Run this each time after restarting the Kernel\n!pip install watson_developer_cloud\n# Run pip install only the first time, once installed on your Spark machine, no need to re-run unless you want to upgrade\n!pip install --upgrade --force-reinstall wordcloud\n!pip install --user --upgrade pixiedust"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import watson_developer_cloud as watson\nfrom botocore.client import Config\nimport ibm_boto3\n\nfrom urllib.request import urlopen \nimport requests\nimport json\nimport io\nfrom os.path import join, dirname\n\nfrom watson_developer_cloud import SpeechToTextV1, NaturalLanguageUnderstandingV1, NaturalLanguageClassifierV1, ToneAnalyzerV3\nfrom watson_developer_cloud.natural_language_understanding.features import (\n    v1 as Features)\n\n\nimport pixiedust\nfrom pixiedust.display import *\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\nimport matplotlib.pyplot as plt\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def set_up_object_storage(credentials_object_storage):\n    endpoints = requests.get(credentials_object_storage['endpoints']).json()\n\n    iam_host = (endpoints['identity-endpoints']['iam-token'])\n    cos_host = (endpoints['service-endpoints']['cross-region']['us']['public']['us-geo'])\n\n    auth_endpoint = \"https://\" + iam_host + \"/oidc/token\"\n    service_endpoint = \"https://\" + cos_host\n\n\n    client = ibm_boto3.client(\n        's3',\n        ibm_api_key_id = credentials_object_storage['apikey'],\n        ibm_service_instance_id = credentials_object_storage['resource_instance_id'],\n        ibm_auth_endpoint = auth_endpoint,\n        config = Config(signature_version='oauth'),\n        endpoint_url = service_endpoint\n       )\n    return client\n\nclient = set_up_object_storage(credentials_os)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Set up Watson STT\n\nspeech_to_text = SpeechToTextV1(\n    username = credentials_stt['username'],\n    password = credentials_stt['password'],\n    url = 'https://stream.watsonplatform.net/speech-to-text/api',\n)\n\ndef get_transcript(audio):\n    transcript = json.dumps(speech_to_text.recognize(audio=audio, content_type='audio/ogg', timestamps=True,\n        word_confidence=True), indent=2)\n    return transcript\n\ndef download_file(path, filename):\n    url = path + filename\n    print(url)\n    r = requests.get(url, stream=True)\n    return r.content\n\ndef analyze_sample(sample):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=sample)['Body'] #http\n    audio = streaming_body.read()\n    text = get_transcript(audio)\n    # client.put_object(Bucket = credentials_os['BUCKET'], Key = sample.split('.')[0] + '_text.json', Body = text) ## Already done for Demo purposes\n    return text\n\ndef visualize(transcript):\n    for result in json.loads(transcript)['results']:\n        print(result['alternatives'][0]['transcript'], result['alternatives'][0]['confidence'])    \n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Setup Watson NLU\nfeatures = {\"concepts\":{},\"entities\":{},\"keywords\":{},\"categories\":{},\"emotion\":{},\"sentiment\":{},\"semantic_roles\":{} }\n\nnatural_language_understanding = NaturalLanguageUnderstandingV1(\n    version = '2017-02-27',\n    username = credentials_nlu['username'],\n    password = credentials_nlu['password']\n)\n\nchunk_size=25 # This CHUNK size is used to disaggregate a transcript \n#e.g. in this case a 290 word transcript would have 10 chunks - 9 with 30 words and 1 with 20 words - approximates 'time domain' for this lab\n\ndef chunk_transcript(transcript, chunk_size):\n    transcript = transcript.split(' ')\n    return [ transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size) ] # chunking data\n\n        \ndef process_text_chunks(text):\n    transcript=''\n    for sentence in json.loads(text)['results']:\n        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n    transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n    return  transcript\n\ndef analyze_transcript_chunks(features, file_name):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key=file_name.split('.')[0]+'_text.json')['Body']\n    transcript=streaming_body.read().decode(\"utf-8\")\n    nlu_analysis={}\n    for chunk in process_text_chunks(transcript):\n        chunk = ' '.join(chunk)\n        print('chunk: ', chunk)\n        nlu_analysis[chunk] = natural_language_understanding.analyze(features, chunk, return_analyzed_text=True, language='en')\n    outfilename = file_name.split('.')[0]+'_NLUchunks.json'\n    print(\"writing file: \", outfilename, \" to cloud object storage\" )\n    # res=client.put_object(Bucket = credentials_os['BUCKET'], Key=outfilename, Body= json.dumps(nlu_analysis)) Already done for Demo purposes\n    return nlu_analysis\n\n\ndef post_analysis_chunks(result):\n    for chunk in result.keys():\n        categories = result[chunk]['categories']\n        print('\\nchunk: ', chunk)\n        for category in categories:\n            print('label: ', category['label'], ', score: ', category['score']) #add table instead of prints"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result = analyze_transcript_chunks(features, file_list[0])\npost_analysis_chunks(result)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Setup Watson Natural Language Classifier\n\nnatural_language_classifier = NaturalLanguageClassifierV1(\n    username = credentials_nlc['username'],\n    password = credentials_nlc['password'])\n\nchunk_size = 25\n# Used to SPLIT up - \"CHUNK\" the aggregate transcript into smaller pieces    \n\ndef process_text(text):\n    transcript=''\n    for sentence in json.loads(text)['results']:\n        transcript = transcript + sentence['alternatives'][0]['transcript'] # concatenate sentences\n    transcript = chunk_transcript(transcript, chunk_size) # chunk the transcript\n    return transcript\n\ndef classify(file_name):\n    streaming_body = client.get_object(Bucket = credentials_os['BUCKET'], Key = file_name.split('.')[0]+'_text.json')['Body']\n    transcript=streaming_body.read().decode(\"utf-8\")\n    analysis = {}\n    for chunk in process_text(transcript):\n        chunk = ' '.join(chunk)\n        analysis[chunk] = natural_language_classifier.classify(credentials_nlc['classifier_id'], chunk)\n    ## client.put_object(Bucket = credentials_os['BUCKET'], Key = file_name.split('.')[0]+'_nlc', Body= json.dumps(analysis)) # Done already for Demo purposes\n    return analysis\n\n\ndef classify_transcript(file_name):\n    status = natural_language_classifier.get_classifier(credentials_nlc['classifier_id'])\n    if status['status'] == 'Available':\n        classes = classify(file_name)\n    return classes"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "analysis = classify_transcript(file_list[0])\nprint(analysis)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Setup Watson Tone Analyzer\n\ntone_analyzer = ToneAnalyzerV3(version = '2016-05-19',\n                               username = credentials_tone['username'],\n                               password = credentials_tone['password'])\n\n\nchunk_size=25\n\ndef analyze_transcript(file_name):\n    transcript = client.get_object(Bucket = credentials_os['BUCKET'], Key = file_name.split('.')[0]+'_text.json')['Body']\n    transcript = transcript.read().decode(\"utf-8\")\n    tone_analysis={}\n    for chunk in process_text(transcript):\n        if len(chunk) > 2:\n            chunk = ' '.join(chunk)\n            tone_analysis[chunk] = tone_analyzer.tone(chunk, content_type='text/plain')\n    # res=client.put_object(Bucket = credentials_os['BUCKET'], Key= file_name.split('.')[0]+'_tone.json', Body = json.dumps(tone_analysis))\n    return tone_analysis\n\ndef print_tones(tones):\n    for tone in tones:\n        print(tone)\n\ndef post_analysis(result):\n    for chunk in result.keys():\n        tone_categories = result[chunk]['document_tone']['tone_categories']\n        print('\\nchunk: ', chunk)\n        for tone_category in tone_categories:\n            print_tones(tone_category['tones'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result = analyze_transcript(file_list[0])\npost_analysis(result) "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Method to parse NLU response file from Cloud Object Storage\n# and return sentiment score, sentiment label, and keywords\n# This method works for the scenario of one NLU call per call (file)\ndef getNLUresponse(COSclient, bucket, files):\n    nlu_results = []\n    for filename in files:\n        # Extract NLU enriched filename from the original file name\n        nlu_filename = filename.split('.')[0]+'_NLU.json'\n        print(\"Processing NLU response from file: \", nlu_filename)\n        streaming_body = COSclient.get_object(Bucket=bucket, Key=nlu_filename)['Body']\n        nlu_response = json.loads(streaming_body.read().decode(\"utf-8\"))\n        #print(json.dumps(nlu_response,indent=2))\n        if nlu_response and nlu_response['sentiment'] \\\n        and nlu_response['sentiment']['document'] and nlu_response['sentiment']['document']['label']:\n            sentiment_score = nlu_response['sentiment']['document']['score']\n            sentiment_label = nlu_response['sentiment']['document']['label']\n            keywords = list(nlu_response['keywords'])\n        else:\n            sentiment_score = 0.0\n            sentiment_label = None\n            keywords = null\n        nlu_results.append((filename,sentiment_score,sentiment_label,keywords))\n    return (nlu_results)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Method to parse NLU Emotion Tone response file from Cloud Object Storage\ndef getChunkNLU(nlu_response):\n    #print(json.dumps(nlu_response,indent=2))\n    if nlu_response and nlu_response['sentiment'] \\\n    and nlu_response['sentiment']['document'] and nlu_response['sentiment']['document']['label']:\n        sentiment_score = nlu_response['sentiment']['document']['score']\n        sentiment_label = nlu_response['sentiment']['document']['label']\n        keywords = list(nlu_response['keywords'])\n    else:\n        sentiment_score = 0.0\n        sentiment_label = None\n        keywords = null\n    \n    return sentiment_score, sentiment_label, keywords\n\n# Method to parse NLU response file from Cloud Object Storage\n# and return sentiment score, sentiment label, and keywords\n# This method handles the scenario when call is broken into multiple chunks\ndef getNLUresponseChunks(COSclient, bucket, files):\n    nlu_results = []\n    print(\"files: \", files)\n    for filename in files:\n        # Extract NLU enriched filename from the original file name\n        nlu_filename = filename.split('.')[0]+'_NLUchunks.json'\n        print(\"Processing NLU response from file: \", nlu_filename)\n        streaming_body = COSclient.get_object(Bucket=bucket, Key=nlu_filename)['Body']\n        nlu_chunks_response = json.loads(streaming_body.read().decode(\"utf-8\"))\n        if nlu_chunks_response and len(nlu_chunks_response)>0:\n            chunkidx = 0\n            for chunk in nlu_chunks_response:\n                chunk_nlu = getChunkNLU(nlu_chunks_response[chunk])\n                print('chunk nlu: ', chunk_nlu)\n                print('type of chunk nlu: ', type(chunk_nlu))\n                chunkidx = chunkidx + 1\n                tmp_results = (filename, chunkidx, chunk_nlu)\n                l = list((filename,chunkidx)) + list(chunk_nlu)\n                nlu_results.append(l)\n        \n    return (nlu_results)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Alternative call to handle the case when the NLU response has been broken into chunks of 25 words each\nnlu_header=['filename','chunkidx','sentiment_score','sentiment_label','keywords']\nnlu_results = getNLUresponseChunks(client, credentials_os['BUCKET'], file_list)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "callcenterlogs_nluDF = spark.createDataFrame(nlu_results, nlu_header)"
        }, 
        {
            "source": "### Sentiment plots using PixieDust\nLeverage PixieDust to plot sentiment labels as a pie-chart showing how many positive, negative, and neutral calls are received.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "## Ignore any records with null sentiment label\ncallcenterlogs_nluDF = callcenterlogs_nluDF.where(col('sentiment_label').isNotNull())\nperlabel_sentimentDF = callcenterlogs_nluDF.groupBy('sentiment_label')\\\n                              .agg(F.count('filename')\\\n                              .alias('num_calls'))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "pieChart", 
                        "valueFields": "sentiment_score", 
                        "keyFields": "sentiment_label"
                    }
                }
            }, 
            "outputs": [], 
            "source": "# Call Pixiedust to visualize sentiment data\ndisplay(callcenterlogs_nluDF)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}